<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Spatial Frequency Preferences in Human Visual Cortex</title>
<meta name="author" content="(Billy Broderick)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="reveal.js/css/reveal.css"/>

<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme"/>

<link rel="stylesheet" href="custom.css"/>
<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'reveal.js/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<meta name="description" content="1st year talk">
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide"><h1>Spatial Frequency Preferences in Human Visual Cortex</h1><h2>Billy Broderick</h2><h2>Simoncelli and Winawer labs</h2><h2>Oct 19, 2017</h2>
</section>
<aside class="notes">
<p>
I'm Billy Broderick and over the past couple months I've been working
on a project with Eero Simoncelli and Jon Winawer on a project looking
at spatial frequency preferences in the human visual cortex. We're
looking at V1, V2, V3, but for this talk I'm going to focus on V1.
</p>

</aside>

<section>
<section id="slide-orgc41a69e">
<h2 id="orgc41a69e"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/einstein.svg" class="org-svg" width="70%" height="70%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
Like many people in this department, I'm interested in how the brain
processes visual stimuli.
</p>

<p>
now historically, most of this work has been done in single
neurons. and you might think if you know how a small handful of
neurons work, you know how the whole population works, that they're
basically the same everywhere. 
</p>

<p>
if you're looking at this picture of Einstein, focusing on Einstein's
nose in the center of the image&#x2026;
</p>

</aside>

</section>
</section>
<section>
<section id="slide-orge51a605">
<h2 id="orge51a605"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/cells_no_change-first.svg" class="org-svg" width="70%" height="70%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
a cell whose receptive field is in your fovea and thus cares about the
center of gaze would see Einstein's mustache
</p>

</aside>

</section>
</section>
<section>
<section id="slide-org211a25a">
<h2 id="org211a25a"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/cells_no_change-all.svg" class="org-svg" width="70%" height="70%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
and cells whose recpetive fields are farther out in the periphery
would see other portions of this image. notice the only thing that
differs is the location of their receptive fields, in every other
respect, they're identical
</p>

<p>
this way of thinking assumes that cells in V1 functions basically as a
convolution, doing the same thing everywhere in the image. this
underlies a lot of our technology: all convolutional neural nets,
which underlie the recent success in object and speech recognition,
have this as the basic first step.
</p>

<p>
but we know this is false
</p>

</aside>

</section>
</section>
<section>
<section id="slide-org4a6e2f8">
<h2 id="org4a6e2f8"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/Freeman2011-RFs-example.svg" class="org-svg" width="100%" height="100%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
at the very least, the size of the receptive field changes across the
visual field, so that neurons at the fovea get a smaller amount of the
image than those at the periphery.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-org2f37695">
<h2 id="org2f37695"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/Freeman2011-RFs-plot.svg" class="org-svg" width="100%" height="100%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
and this doesn't vary arbitrarily across the visual field, there's in
fact a linear relationship between the eccentricity, or distance from
the fovea / center of gaze, and the size of the receptive field.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-org1f86ff8">
<h2 id="org1f86ff8"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/cells_size_scale-all.svg" class="org-svg" width="70%" height="70%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
so to return to our Einstein image, this is at least true, that
peripheral cells see larger chunks of the image than foveal ones.
</p>

<p>
but this leads to a natural follow up question: what else changes
across the visual field? what tuning properties of the cell differ as
you move towards the periphery?
</p>

<p>
I'm going to focus on one property, spatial frequency, which is
roughly equivalent to the speed of change in an image.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-org4707565">
<h2 id="org4707565">Spatial frequency</h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/spatial-frequency.svg" class="org-svg" width="100%" height="100%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
this has traditionally been studied by using 2d sine gratings like
these, which drive responses in V1 very well and have constant
frequencies across the image. Here, the gratings go from low frequency
/ slow changes on the left to higher and higher as you move to the
right.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-org45c6d05">
<h2 id="org45c6d05"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/macaque-SF-half.svg" class="org-svg" width="110%" height="110%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
and if you show these images to a macaque while recording from neurons
in its V1, you'll get a response curve like this. Here, spatial
frequency is on the x-axis and the normalized response is on the y
axis and we see that we get a tuning curve. thus the neuron has a peak
/ preferred spatial frequency.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-org689007a">
<h2 id="org689007a"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/macaque-SF.svg" class="org-svg" width="110%" height="110%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
but we know that for different neurons, even near each other in the
cortex, neurons have a diversity of spatial frequency tunings, with
their peaks at different spatial frequencies.
</p>

<p>
so the question is, how does this diversity of spatial frequency
tunings interact with the visual field map in V1. does every chunk of
cortex look basically like this, or do the tunings depend on the
eccentricity in the same way that receptive field size does?
</p>

</aside>

</section>
</section>
<section>
<section id="slide-org95aa87c">
<h2 id="org95aa87c"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/cells_size_scale-all.svg" class="org-svg" width="70%" height="70%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
to return to Einstein, if instead of the size of the receptive field I
show the preferred spatial frequency&#x2026;
</p>

</aside>

</section>
</section>
<section>
<section id="slide-orgc0c9640">
<h2 id="orgc0c9640"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/cells_size_scale-sf.svg" class="org-svg" width="70%" height="70%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
do cells that care about different parts of the visual field have
approximately the same spatial frequency tuning, even though they have
larger receptive fields&#x2026;
</p>

</aside>

</section>
</section>
<section>
<section id="slide-orga2e9904">
<h2 id="orga2e9904"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/cells_scaling_blur-sf.svg" class="org-svg" width="70%" height="70%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
or does that spatial frequency tuning vary along with the receptive
field size, such that cells are tuned for lower and lower spatial
frequencies as you move from the fovea out to the periphery.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-orge35e7ca">
<h2 id="orge35e7ca"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/SF-hypotheses.svg" class="org-svg" width="100%" height="100%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
to put what I just said onto a set of axes, here I'm plotting these
two hypotheses, which I'm going to call the constant and scaling
hypotheses, as the peak spatial frequency vs. the eccentricity of the
receptive field.
</p>

<p>
in the constant hypothesis, it doesn't matter where you are in the
visual field, the cells prefer the same spatial frequency. in the
scaling hypothesis, the preferred spatial frequency drops inversely
with the distance from the fovea.
</p>

<p>
there's some existing data on this, but not as much as you may think
that looks at this systematically, and those that I found did not
agree with each other.
</p>

<p>
I would like to analyze how visual cortex responds to different
spatial frequencies across the map in the primary visual cortex. in
order to do this systematically, we need fMRI, which allows us to
efficiently measure the response in V1 across all eccentricities in
response to images.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-orgfb449ad">
<h2 id="orgfb449ad"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/Benson-retinotopy.svg" class="org-svg" width="100%" height="100%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
just as we can measure RFs in neurons, we can measure population
receptive fields in fMRI, and in a similar manner. the idea is that
for each patch of cortex, we find what patch of the visual field best
drives response, so we are performing retinotopy experiments.
</p>

<p>
we typically describe these using polar coordinates, so we talk about
the eccentricity (which, as I explained, is the distance from the
center of gaze)  and the polar angle. Here, I'm showing those two
values plotted both in the visual field and on the cortex.
</p>

<p>
for my experiment, I'm using a method from Noah Benson, a postdoc in
Jon's lab, which is able to predict these retinotopy models fairly
well from a person's anatomy alone.
</p>

</aside>


</section>
</section>
<section>
<section id="slide-org1678bab">
<h2 id="org1678bab"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/stimulus_example.svg" class="org-svg" width="60%" height="60%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
so now, finally, we can talk about my experiment.
</p>

<p>
we construct these stimuli to directly encode the scaling
hypothesis. they're constructed so that the spatial frequency in the
image drops off as you move away from the fovea inversely with the
distance from the fovea. and this means that the scaling hypothesis
predicts that the response to this image in V1 across all
eccentricities will be the same. that's because the difference in
preferred spatial frequencies between a voxel whose population
receptive field is centered at 2 degrees and one whose population
receptive field is centered at 8 degrees will be exactly the
difference between the spatial frequency in this image at those two
locations / that lie within their population receptive fields.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-org45d40b3">
<h2 id="org45d40b3"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/stimuli-overview.svg" class="org-svg" width="100%" height="100%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
we construct a whole bunch of stimuli like this. we use gratings like
this because they drive V1 well, allow us to avoid edge artifacts
while still scaling with eccentricity, and a have a variety of local
orientation properties, which we also know is important for V1. we
also vary their base frequency, so we can measure responses to a
variety of spatial frequencies at each location.
</p>

<p>
I'm now going to show you the experiment, slowed down so you can see
what's going on and because I'm a little weird it could trigger a
photosensitive epilectic seizure &#x2013; if flashes of light bother you,
you should still probably close your eyes and I'll let you know when
you can open them again.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-orgdbcb653">
<h2 id="orgdbcb653"></h2>

<div class="figure">
<p><img src="../images/experiment-example-trial.gif" alt="experiment-example-trial.gif" width="130%" height="130%" />
</p>
</div>

<aside class="notes">
<p>
for the experiment, we have people lie in the scanner and stare at
these stimuli for a long time. each class of stimuli contains 8 images
which differ in their phase but have the same frequency
information. the images flash on and off for 300/200 msecs. (both the
phase differences and on/off periods are done to reduce effect of
adaptation). people are shown these images sequentially while fixating
in the center at a digit stream, doing a one-back task as a distractor
to maintain fixation and a constant cognitive state. there are 52 of
these different stimulus categories, covering a wide range of
frequencies, and we intersperse 10 "blank classes" to act as baselines
(distractor task still happens).
</p>

<p>
that's one run, and we have 9 of these, with the stimulus class order
randomized across runs
</p>

<p>
after some analysis, we end up with a number for the response of each
voxel in the cortex to each of our stimuli.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-org58cb3f4">
<h2 id="org58cb3f4">Eccentricity bands</h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/V1-eccentricity-bands-blank.svg" class="org-svg" width="90%" height="90%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
okay, you can open your eyes again.
</p>

<p>
but we're only interested in V1, so we only look at voxels that lie
there.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-org9b2ddfe">
<h2 id="org9b2ddfe">Eccentricity bands</h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/V1-eccentricity-bands.svg" class="org-svg" width="90%" height="90%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
we then reduce it further by binning the voxels in V1 by their
eccentricities, averaging together those response numbers for each
voxel whose population receptive fields lie within eccentricities two
to three, three to four etc, so we have one number for each of
those. and then we fit log Gaussian tuning curves to them and examine
the results.
</p>

<p>
before I show them to you, I'd like to return to our possible
hypotheses.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-org1a6a2d9">
<h2 id="org1a6a2d9"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/SF-hypotheses.svg" class="org-svg" width="100%" height="100%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
so you remember our scaling and constant hypotheses.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-org03c7064">
<h2 id="org03c7064"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/SF-hypotheses-constant.svg" class="org-svg" width="100%" height="100%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
in the constant hypothesis, tuning doesn't change across the visual
field, so if we plot the responses as a function of the local spatial
frequency, that is, as a function of the spatial frequency in the
patch of the image that a voxel is responding to, the responses of
voxels at all eccentricities should be basically on top of each other,
since their tuning is all the same. on the other hand, if we plot the
response as a function of the stimuli, the responses will be all
splayed out, because each stimulus has a different spatial frequency
in a given portion of the image
</p>

</aside>

</section>
</section>
<section>
<section id="slide-orgd1e35b6">
<h2 id="orgd1e35b6"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/SF-hypotheses-scaling.svg" class="org-svg" width="100%" height="100%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
the scaling hypothesis, conversely, predicts that if we plot the
responses as a function of the stimuli, all eccentricities will lie on
top of each other, because we constructed our stimuli in a particular
way and if we plot the responses as a function of the local spatial
frequency, they'll be spread out because they all have different
tuning.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-org1420d5f">
<h2 id="org1420d5f"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/SF-wl_subj001-results_hyp.svg" class="org-svg" width="100%" height="100%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
and this is more or less what we see
</p>

<p>
in this top plot, we're plotting it as a function of local spatial
frequency and we see the curves are shifted around, with different
eccentricities having their peaks at different frequencies.
</p>

<p>
in the bottom, we're plotting it as a function of the stimulus image,
and we see the lines are all basically on top of each other, as the
scaling hypothesis predicted.
</p>

<p>
however, there is this interesting amplitude effect and some
divergences from the curve &#x2013; which, though it's hard to see with the
fit line, are basically consistent across eccentricities as
well. there's also the possibility of edge effects going on here and I
need to decide figure out how to handle that
</p>

<p>
it can be kind of hard to see where the peaks of the curves are
because of this amplitude effect, so I'm going to replot this, taking
the peak spatial frequency of each curve and plotting it against the
spatial frequency.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-org6bf1d2c">
<h2 id="org6bf1d2c"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/conclusion.svg" class="org-svg" width="100%" height="100%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
the peak spatial frequency falls off inversely with receptive field
size. I had to rescale the beginning and end of this curve here, and
it doesn't exactly follow it, but it's a hyperbola, for sure.
</p>

<p>
so in conclusion
</p>

<ul>
<li>Receptive field varies systematically across visual field.</li>
<li>V1 neurons have different spatial frequency tuning.</li>
<li>Does V1 spatial freuqeny tuning differ across visual field?</li>
<li>Preliminary results suggest preferred spatial frequency shrinks
inversely proportional to receptive field size.</li>

</ul>

<p>
I'd also like to emphasize, though I've formulated this talk as being
all about these two qualitative hypotheses, mainly because of time
constraints, that's not the actual point of this project. The ultimate
goal is to use this data to get careful measurements of these spatial
frequency tunings and use that to create forward models of the visual
system, which can predict the responses of voxels across the visual
cortex to arbitrary visual stimuli.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-orgf87687b">
<h2 id="orgf87687b">Thanks!</h2>
<ul>
<li>Jon Winawer</li>
<li>Eero Simoncelli</li>
<li>Noah Benson</li>
<li>Eline Kupers</li>
<li>Winawer and Simoncelli labs</li>

</ul>

</section>
</section>
<section>
<section id="slide-org21cb7cf">
<h2 id="org21cb7cf"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/conclusion-plus-Olsson.svg" class="org-svg" width="100%" height="100%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
I didn't show our participants images that encode the constant
hypothesis, but Catherine Olsson did a pilot study a little bit ago
that did (showed people bandpassed noise) and here I've plotted the
peak of her log Gaussian tuning curves with my results. they look
pretty similar, though there's a slight shift.
</p>

<p>
one of the things I would like to do is examine how the stimuli used
affects these results and so far I only showed you the results for one
type of those stimuli, the circular ones, so it's possible this shift
will make sense later.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-orgaafbcc4">
<h2 id="orgaafbcc4"></h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/SF-Summary-details-with-data.svg" class="org-svg" width="100%" height="100%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
existing data conflicts.
</p>

<p>
I looked across all the papers that I could find that measured this
systematically (if you know, please tell me!) and this is what I
found. these are all done in humans with fMRI, but I would love to see
similar efforts in physiology as well. these studies generally used
different types of stimuli and were aware of each other, citing each
other, but they cited each other approvingly, saying "this generally
matches", instead of talking about differences or why they could be.
</p>

<p>
the basic intuition of lower spatial frequencies in the periphery
rather than the fovea, holds. and they basically all agree that the
drop-off should be exponential rather than linear (these are
approximated / averaged from the original data)
</p>

<p>
but results at a given eccentricity in V1 span about two octaves, or
one if we discount the Kay results, unsure how this varies across
subjects, not sure exact form of relationship between, or the effect
of different stimuli types
</p>

</aside>

</section>
</section>
<section>
<section id="slide-orgb19e73f">
<h2 id="orgb19e73f">Psychophysical evidence of different spatial frequency channels</h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/Blakemore1969-exist-neuron.svg" class="org-svg" width="80%" height="80%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
adaptation experiment from Blakemore and Campbell
</p>

</aside>

</section>
</section>
<section>
<section id="slide-orgb4885cc">
<h2 id="orgb4885cc">What is spatial frequency?</h2>

<div class="figure">
<p><object type="image/svg+xml" data="../images/einstein.svg" class="org-svg" width="60%" height="60%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
since I've going to spend this talk discussing spatial frequency, I
figured it'd be a good idea to start by describing what it is. I'm
going to be discussing only grayscale images in my talk, so when I say
frequency, I'm not referring to light wavelength at all.
</p>

<p>
spatial frequency is a way to describe (and quantify) the changes (in
pixel intensity) in an image. at high frequency, you have very quick
changes (over a short period), like you find at edges. at low
frequency, you have very gradual changes (over a long period), like in
the overall brightness of the image from one side to another
</p>

</aside>

</section>
<section id="slide-org3b2745d">
<h3 id="org3b2745d">What is spatial frequency?</h3>

<div class="figure">
<p><object type="image/svg+xml" data="../images/einsteinSF-two.svg" class="org-svg" width="100%" height="100%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
beyond just waving my hands about and describing changes in an image,
we can actually break an image down into its frequency parts. here, we
see I've extracted out the high frequency, which highlights the edges,
and separated it out from the lower frequency, which looks like a
blurrier version of the image
</p>

</aside>

</section>
<section id="slide-org0f3bc2f">
<h3 id="org0f3bc2f">What is spatial frequency?</h3>

<div class="figure">
<p><object type="image/svg+xml" data="../images/einsteinSF.svg" class="org-svg" width="100%" height="100%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
and we do this not just twice but on and on, separating out the
highest from the lowest frequencies, and getting all those in the
middle as well.
</p>

<p>
this turns out to be very important. I'm just going to assert that,
for any image, I can split it up in this way to find out how much of a
given frequency it has. this is a general principle that holds for all
images and, in fact, all signals, whether that's images or sounds or
movies, or anything
</p>

</aside>
</section>
<section id="slide-orgd7ff228">
<h3 id="orgd7ff228">How do neurons calculate spatial frequency?</h3>

<div class="figure">
<p><object type="image/svg+xml" data="../images/einsteinSF.svg" class="org-svg" width="100%" height="100%">
Sorry, your browser does not support SVG.</object>
</p>
</div>

<aside class="notes">
<p>
neurons <i>can</i> calculate lower spatial frequency by averaging over
larger and larger area.
</p>

</aside>

</section>
<section id="slide-orgb0e0301">
<h3 id="orgb0e0301">What does this have to do with biological vision?</h3>
<aside class="notes">
<p>
but what I've told you so far has nothing to do with vision or
neuroscience, only electrical engineering. so what's the point of it?
</p>

<p>
the point is, we have a hypothesis that vision works in a similar way:
that we have different channels for different spatial frequencies,
representing each of them separately in the brain. there's both
physiological and psychophysical evidence of this; I'm just going to
show physiological for the sake of time.
</p>

</aside>




</section>
</section>
<section>
<section id="slide-org867c860">
<h2 id="org867c860">Vision differs across visual field</h2>

<div class="figure">
<p><img src="../images/Robson-CSF-hf.gif" alt="Robson-CSF-hf.gif" width="80%" height="80%" />
</p>
</div>

<aside class="notes">
<p>
it does, and I have a brief demonstration of that. again, there's
behavioral data on this from other people, which I won't go into here.
</p>

<p>
so I want you to follow this moving patch. as it moves vertically up
the image, you see it gets fainter and fainter, but you can still see
it most of the way up if you track it with your eyes (this will vary
based on how good your eyes and where you are in the room).
</p>

</aside>

</section>
<section id="slide-org3df3c24">
<h3 id="org3df3c24">Vision differs across visual field</h3>

<div class="figure">
<p><img src="../images/Robson-CSF-hf-dot.gif" alt="Robson-CSF-hf-dot.gif" width="80%" height="80%" />
</p>
</div>

<aside class="notes">
<p>
however, if you fixate on this dot over here, while still attending to
the moving patch, you'll lose sight of it around halfway up (again,
specifics will vary)
</p>

<p>
this is because in this example, you're viewing it in your periphery,
where your sensitivity is much worse compared to the fovea, where we
were viewing it in the last slide
</p>

</aside>
</section>
</section>
</div>
</div>
<p> Created by WFB. </p>
<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: false,
progress: true,
history: false,
center: true,
slideNumber: false,
rollingLinks: false,
keyboard: true,
overview: true,
margin: 0.10,
minScale: 0.50,
maxScale: 2.50,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'none', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: 'default',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
});
</script>
</body>
</html>
